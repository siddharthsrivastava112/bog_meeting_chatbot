# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16yX6HlcOCrXKFSVFZQhApkGhM1znGN5d
"""

!rm -rf /opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info
!pip install -q llama-index llama-index-core llama-parse neo4j

import nest_asyncio

# Apply the nest_asyncio to allow the notebook to run asynchronous code without issues.
nest_asyncio.apply()

!pip install -q llama-index-llms-groq

GROQ_API_KEY ="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"
from llama_index.llms.groq import Groq

llm = Groq(api_key=GROQ_API_KEY, model="llama3-70b-8192")

from google.colab import drive
from llama_parse import LlamaParse
import re

# STEP 1: Mount Google Drive
drive.mount('/content/drive')

# STEP 2: Parse the PDF with LlamaParse
LLAMA_CLOUD_API_KEY = "llx-cyobizmP15x14Gfj5fSMho5Dz1JV0UZ7N061FaRfWI9rXf2d"
file_path = '/content/drive/MyDrive/data/ug.pdf'

parser = LlamaParse(api_key=LLAMA_CLOUD_API_KEY, result_type="markdown")
documents = parser.load_data(file_path)

# STEP 3: Convert Markdown Tables to Natural Language Sentences
def convert_markdown_table_to_text(markdown_text):
    lines = markdown_text.split('\n')
    new_lines = []
    i = 0
    while i < len(lines):
        line = lines[i]
        # Detect a table header
        if '|' in line and i + 2 < len(lines) and '---' in lines[i + 1]:
            headers = [h.strip() for h in lines[i].split('|') if h.strip()]
            i += 2  # skip the header and separator
            while i < len(lines) and '|' in lines[i]:
                values = [v.strip() for v in lines[i].split('|') if v.strip()]
                if len(values) == len(headers):
                    sentence = ", ".join([f"{headers[j]}: {values[j]}" for j in range(len(headers))])
                    new_lines.append(sentence)
                i += 1
        else:
            new_lines.append(line)
            i += 1
    return '\n'.join(new_lines)

# STEP 4: Apply to all documents
for idx, doc in enumerate(documents):
    print(f"\n=== Document {idx + 1} (Processed) ===\n")
    processed_text = convert_markdown_table_to_text(doc.text)
    print(processed_text[:1500])  # print first 1500 characters



print(documents[0].text[:1000])

from llama_index.core.node_parser import MarkdownElementNodeParser

# Process the documents and returns the parsed nodes.
node_parser = MarkdownElementNodeParser(llm=llm, num_workers=8)
nodes = node_parser.get_nodes_from_documents(documents)

from llama_index.llms.groq import Groq  # Example, you can use any LLM of your choice

# Replace with your actual Groq API key
llm = Groq(model="llama3-70b-8192", api_key="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")

node_parser = MarkdownElementNodeParser(llm=llm, num_workers=8)
nodes = node_parser.get_nodes_from_documents(processed_documents)

# STEP 6: Optional: Print the first node content for verification
print(nodes[0].get_content()[:500])  #



print(f"Nodes Type: \n{type(nodes)}\n")
print(f"Nodes Elements Type: \n{type(nodes[0])}\n")
print(f"Nodes Len: \n{len(nodes)}\n")

for i in range(7):
    print(f"Node {i} Content:")
    print(nodes[i].get_content())
    print("\n" + "=" * 115 + "\n")

# Extract base nodes and objects from a given list of nodes.
base_nodes, objects = node_parser.get_nodes_and_objects(nodes)
print(f"Base Nodes Type: \n{type(base_nodes)}\n")
print(f"Base Node Elements Type: \n{type(base_nodes[0])}\n")
print(f"Base Nodes Len: \n{len(base_nodes)}\n")

for i in range(55):
    print(f"Base Node {i} Content:")
    print(base_nodes[i].get_content())
    print("\n" + "=" * 115 + "\n")

import json

# Constants used to detect table-related nodes
TABLE_REF_SUFFIX = '_table_ref'
TABLE_ID_SUFFIX  = '_table'

# Print general info about the object list
print(f"\nObjects Type: {type(objects)}")
print(f"Each Object Type: {type(objects[0]) if objects else 'N/A'}")
print(f"Total Objects: {len(objects)}\n")

# Iterate over all nodes
for idx, node in enumerate(objects):
    print(f"\n{'=' * 40} Node {idx + 1} {'=' * 40}")

    # Display basic node info
    print(f"Node ID: {node.node_id}")
    print(f"Type: {node.get_type()}")
    print(f"Class: {node.class_name()}")
    print(f"Content (first 200 chars):\n{node.get_content()[:200]}")
    print(f"Metadata:\n{node.metadata}")
    print(f"Extra Info:\n{node.extra_info}")

    # Parse JSON for detailed info
    try:
        node_json = json.loads(node.json())
        start_idx = node_json.get("start_char_idx")
        end_idx = node_json.get("end_char_idx")
        print(f"Start Index: {start_idx}")
        print(f"End Index: {end_idx}")
    except Exception as e:
        print(f"Could not parse JSON: {e}")

    # Check if this is a table reference node
    if node.node_id.endswith(TABLE_REF_SUFFIX):
        print("\nðŸŸ¦ Table Reference Node Detected")

        # Attempt to access the next node (should be the table)
        next_node = node.next_node
        if next_node:
            try:
                table_json = json.loads(next_node.json())
                metadata = table_json.get("metadata", {})
                table_summary = metadata.get("table_summary", "No summary found")
                table_df = metadata.get("table_df", "No table_df found")

                print(f"\nðŸ“‹ Table Summary:\n{table_summary}")
                print(f"\nðŸ“Š Table Content:\n{table_df}")
            except Exception as e:
                print(f"Error extracting table info: {e}")
        else:
            print("âš ï¸ No next_node found (table content missing?)")

    print("\n")

NEO4J_PASSWORD = "OYX-RODrpLlzoDhrEa5vPZ7qhzMiK20No8UIYw3gn48"
from neo4j import GraphDatabase

NEO4J_URL = "neo4j+s://1a7a75b4.databases.neo4j.io"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = NEO4J_PASSWORD
NEO4J_DATABASE = "neo4j"

# Create a Neo4j driver instance.
driver = GraphDatabase.driver(
        NEO4J_URL,
        database=NEO4J_DATABASE,
        auth=(NEO4J_USER, NEO4J_PASSWORD)
)
def initializeNeo4jSchema():
    # List of Cypher commands to create constraints and indexes.
    cypher_schema = [
        # Create a unique constraint on the 'key' property for nodes labeled 'Section'.
        "CREATE CONSTRAINT sectionKey IF NOT EXISTS FOR (c:Section) REQUIRE (c.key) IS UNIQUE;",
        # Create a unique constraint on the 'key' property for nodes labeled 'Chunk'.
        "CREATE CONSTRAINT chunkKey IF NOT EXISTS FOR (c:Chunk) REQUIRE (c.key) IS UNIQUE;",
        # Create a unique constraint on the 'url_hash' property for nodes labeled 'Document'.
        "CREATE CONSTRAINT documentKey IF NOT EXISTS FOR (c:Document) REQUIRE (c.url_hash) IS UNIQUE;",
        # Create a vector index on the 'value' property of 'Embedding' nodes.
        "CREATE VECTOR INDEX `chunkVectorIndex` IF NOT EXISTS FOR (e:Embedding) ON (e.value) OPTIONS { indexConfig: {`vector.dimensions`: 384, `vector.similarity_function`: 'cosine'}};"
    ]

    # Open a session with the Neo4j database.
    with driver.session() as session:
        # Iterate over each Cypher command in the list and execute them in the session.
        for cypher in cypher_schema:
            session.run(cypher)

    driver.close()
initializeNeo4jSchema()

print("Start saving documents to Neo4j...")

# Initialize a counter to keep track of how many documents have been processed.
i = 0

# Open a session to the Neo4j database.
with driver.session() as session:
    for doc in documents:
        # Define a Cypher query that uses the MERGE statement to ensure that a document with a specific 'url_hash' either exists or is created.
        # 'MERGE' looks for a 'Document' node with a 'url_hash' matching 'doc_id'. If it exists, it does nothing; if not, it creates it.
        # 'ON CREATE' sets the 'url' property of the node when a new node is created.
        cypher = "MERGE (d:Document {url_hash: $doc_id}) ON CREATE SET d.url=$url;"

        # Execute the Cypher query, passing in the document's ID and URL as parameters.
        session.run(cypher, doc_id=doc.doc_id, url=doc.doc_id)

        i = i + 1
    session.close()
print(f"{i} documents saved.\n")

print("Start saving nodes to Neo4j...")

# Counter to keep track of how many nodes have been saved.
i = 0

# Open a session with the Neo4j database.
with driver.session() as session:
    for node in base_nodes:
        # MERGE clause ensures that a node of type 'Section' with a specified key exists or creates it if it doesn't.
        cypher  = "MERGE (c:Section {key: $node_id})\n"
        # FOREACH clause sets multiple properties on the Section node only if its 'type' property is currently NULL.
        cypher += " FOREACH (ignoreMe IN CASE WHEN c.type IS NULL THEN [1] ELSE [] END |\n"
        cypher += "     SET c.hash = $hash, c.text=$content, c.type=$type, c.class=$class_name, c.start_idx=$start_idx, c.end_idx=$end_idx)\n"
        # WITH clause passes the Section node to the next part of the query.
        cypher += " WITH c\n"
        # MATCH clause finds a Document node with a specific url_hash.
        cypher += " MATCH (d:Document {url_hash: $doc_id})\n"
        # MERGE clause creates a relationship indicating the Section node is part of the Document node.
        cypher += " MERGE (d)<-[:HAS_DOCUMENT]-(c);"

        # Convert the JSON string of the node to a dictionary to access its properties.
        node_json = json.loads(node.json())

        # Execute the Cypher query with parameters extracted from the node object and its JSON data.
        session.run(
            cypher, node_id=node.node_id, hash=node.hash, content=node.get_content(), type='TEXT', class_name=node.class_name(),
            start_idx=node_json['start_char_idx'], end_idx=node_json['end_char_idx'], doc_id=node.ref_doc_id
        )

        # If the node has a 'next_node', create a relationship from the next node to this node.
        if node.next_node is not None:
            cypher  = "MATCH (c:Section {key: $node_id})\n"
            cypher += "MERGE (p:Section {key: $next_id})\n"
            cypher += "MERGE (p)<-[:NEXT]-(c);"

            session.run(
                cypher, node_id=node.node_id, next_id=node.next_node.node_id
            )

        # If the node has a 'prev_node', create a NEXT relationship pointing from this node to the previous node.
        if node.prev_node is not None:
            cypher  = "MATCH (c:Section {key: $node_id})\n"
            cypher += "MERGE (p:Section {key: $prev_id})\n"
            cypher += "MERGE (p)-[:NEXT]->(c);"

            # Adjust the ID for the previous node if it is a special type (such as a table).
            if node.prev_node.node_id[-1 * len(TABLE_ID_SUFFIX):] == TABLE_ID_SUFFIX:
                prev_id = node.prev_node.node_id + '_ref'
            else:
                prev_id = node.prev_node.node_id

            session.run(
                cypher, node_id=node.node_id, prev_id=prev_id
            )

        i = i + 1
    session.close()
print(f"{i} nodes saved.")

print("Start saving objects to Neo4j...")

# Initialize a counter to track the number of objects processed.
i = 0

# Open a session with the Neo4j database.
with driver.session() as session:
    for node in objects:
        # Deserialize the JSON representation of the node to a Python dictionary.
        node_json = json.loads(node.json())

        # Check if the current node is a table by looking for a specific suffix in its node ID.
        # If the object is a Table, then the 'ref_table' object is created as a Section, and the 'table' object as a Chunk.
        if node.node_id[-1 * len(TABLE_REF_SUFFIX):] == TABLE_REF_SUFFIX:
            # If there is a next node, process it as the actual table object.
            if node.next_node is not None:
                next_node = node.next_node
                obj_metadata = json.loads(str(next_node.json()))

                # Cypher to merge or create a Section node for the table reference.
                cypher  = "MERGE (s:Section {key: $node_id})\n"
                # Continue the query to merge or create a Chunk node for the actual table content.
                cypher += "WITH s MERGE (c:Chunk {key: $table_id})\n"
                # If the Chunk node's type is NULL, set several properties.
                cypher += " FOREACH (ignoreMe IN CASE WHEN c.type IS NULL THEN [1] ELSE [] END |\n"
                cypher += "     SET c.hash = $hash, c.definition=$content, c.text=$table_summary, c.type=$type, c.start_idx=$start_idx, c.end_idx=$end_idx )\n"
                # Create a relationship indicating that the Chunk is under the Section.
                cypher += " WITH s, c\n"
                cypher += " MERGE (s) <-[:UNDER_SECTION]- (c)\n"
                # Link the Section to its corresponding Document.
                cypher += " WITH s MATCH (d:Document {url_hash: $doc_id})\n"
                cypher += " MERGE (d)<-[:HAS_DOCUMENT]-(s);"

                # Execute the Cypher query with parameters filled from node and metadata information.
                session.run(
                    cypher, node_id=node.node_id, hash=next_node.hash, content=obj_metadata['metadata']['table_df'], type='TABLE',
                    start_idx=node_json['start_char_idx'], end_idx=node_json['end_char_idx'], doc_id=node.ref_doc_id,
                    table_summary=obj_metadata['metadata']['table_summary'], table_id=next_node.node_id
                )

            # If a previous node exists, establish a relationship to maintain the document structure.
            if node.prev_node is not None:
                cypher  = "MATCH (c:Section {key: $node_id})\n"
                cypher += "MERGE (p:Section {key: $prev_id})\n"
                cypher += "MERGE (p)-[:NEXT]->(c);"

                # Modify the previous node ID if it ends with a specific suffix.
                if node.prev_node.node_id[-1 * len(TABLE_ID_SUFFIX):] == TABLE_ID_SUFFIX:
                    prev_id = node.prev_node.node_id + '_ref'
                else:
                    prev_id = node.prev_node.node_id

                # Execute the Cypher query to link the current node to the previous one.
                session.run(cypher, node_id=node.node_id, prev_id=prev_id)

        i = i + 1
    session.close()
print(f"{i} objects saved.")

print("Start creating chunks for each TEXT Section...")

# Open a session with the Neo4j database.
with driver.session() as session:
    # Cypher query starts by matching all nodes labeled 'Section' with a property type of 'TEXT'.
    cypher  = "MATCH (s:Section) WHERE s.type='TEXT' \n"
    # Begins a subquery to process each 'Section' node individually.
    cypher += "WITH s CALL {\n"
    # Splits the text of the section into paragraphs based on newline characters.
    cypher += "WITH s WITH s, split(s.text, '\n') AS para\n"
    # Creates an array of indices for each paragraph to use later.
    cypher += "WITH s, para, range(0, size(para)-1) AS iterator\n"
    # Unwinds the iterator to process each paragraph individually; 'i' is the index of each paragraph.
    cypher += "UNWIND iterator AS i WITH s, trim(para[i]) AS chunk, i WHERE size(chunk) > 0\n"
    # Creates a new 'Chunk' node for each paragraph that contains text. The key of each chunk is a combination of the section key and its index.
    cypher += "CREATE (c:Chunk {key: s.key + '_' + i}) SET c.type='TEXT', c.text = chunk, c.seq = i \n"
    # Creates a relationship UNDER_SECTION from each chunk to its parent section.
    cypher += "CREATE (s) <-[:UNDER_SECTION]-(c) } IN TRANSACTIONS OF 500 ROWS ;"

    # Executes the assembled Cypher query within the open session.
    session.run(cypher)
    session.close()

print("=================DONE====================")
driver.close()

!pip install -q sentence-transformers
from sentence_transformers import SentenceTransformer

# Initialize the SentenceTransformer model for generating text embeddings.
model = SentenceTransformer('all-MiniLM-L6-v2')

def LoadEmbedding(label, prop):
    # Establish a connection to the Neo4j database.
    driver = GraphDatabase.driver(NEO4J_URL, auth=(NEO4J_USER, NEO4J_PASSWORD), database=NEO4J_DATABASE)

    # Open a session with the database.
    with driver.session() as session:
        # Run a Cypher query to fetch all nodes with a specified label and retrieve their IDs and text properties.
        result = session.run(f"MATCH (ch:{label}) RETURN id(ch) AS id, ch.{prop} AS text")

        # Initialize a counter to track the number of processed nodes.
        count = 0

        # Iterate over the result set from the query.
        for record in result:
            # Extract the node ID from the record.
            id = record["id"]
            # Extract the text content from the specified property.
            text = record["text"]

            # Generate the embedding for the text using the SentenceTransformer model.
            embedding = model.encode(text).tolist()

            # Prepare a Cypher query to create an 'Embedding' node with the generated embedding,
            # and establish a 'HAS_EMBEDDING' relationship from the original node to this new Embedding node.
            cypher = "CREATE (e:Embedding) SET e.key=$key, e.value=$embedding, e.model=$model"
            cypher = cypher + " WITH e MATCH (n) WHERE id(n) = $id CREATE (n) -[:HAS_EMBEDDING]-> (e)"

            # Execute the Cypher query to update the database.
            session.run(cypher, key=prop, embedding=embedding, id=id, model='all-MiniLM-L6-v2')

            count += 1
        session.close()

        print(f"\nProcessed {count} {label} nodes for property @{prop}.")

LoadEmbedding("Chunk", "text")

!pip install -q groq
from groq import Groq
import os

# Initialize the Groq client with an API key.
client = Groq(
    api_key=GROQ_API_KEY
)

# Define a function to handle querying and text completion.
def query(query):
    embedding = model.encode(query).tolist()

    # Establish a connection to a Neo4j database using the GraphDatabase driver.
    with GraphDatabase.driver(NEO4J_URL, auth=(NEO4J_USER, NEO4J_PASSWORD)) as driver:
        cypher_query = """
        CALL db.index.vector.queryNodes('chunkVectorIndex', 6, $queryEmbedding)
        YIELD node, score
        MATCH (c:Chunk)-[:HAS_EMBEDDING]->(node)
        RETURN c.text as Text, score as Score
        """

    # Execute the Cypher query with the embedding as a parameter.
    records, summary, keys = driver.execute_query(
        cypher_query,
        queryEmbedding=embedding,
        database_="neo4j"
    )
    driver.close()

    # Aggregate the text content from the graph query results.
    graph_retrieved_content = ""

    for record in records:
        graph_retrieved_content += record['Text'] + "\n"

    # Use the Groq API client to generate a chat completion.
    chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "system",
             "content": (
                "You are MNNNIT Allahabad ordinance assistant and handle all rules related to UG and PG. "
                "You receive queries in Spanish and always respond in English accurately, "
                "thinking step by step and ensuring there are no spelling or grammatical mistakes. "
                "Your main task is to search and summarize information based on a series of questions and answers:\n\n"
                f"{graph_retrieved_content}\n\n"
                f"I want you to build a step-by-step response or analysis about \"{query}\", "
                "that reflects the content of the text. The response should be as complete as possible."
            )
        },
        {
            "role": "user",
            "content": f"{query}"
        }
    ],
    model="llama3-70b-8192"
    )

    print(chat_completion.choices[0].message.content)

query("at what marks would i be assigned A grade out of 100")